---
title: "Codebook_HAR"
author: "seffo"
date: "2/20/2020"
output: html_document
---

# Getting and Cleaning Data
## Codebook: Human Activity Recognition Data

This repository contains the source files for the final project og the Getting and Cleaning Data course in the John's Hopkins Data Science specialization at Coursera.

## Description

As per the project description in the course:

"The purpose of this project is to demonstrate your ability to collect, work with, and clean a data set. The goal is to prepare tidy data that can be used for later analysis. (...)

One of the most exciting areas in all of data science right now is wearable computing - see for example this article . Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. The data linked to from the course website represent data collected from the accelerometers from the Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained:

http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones"


## Files

* run_analysis.R : Self-sufficient R source code file that takes care of 1) creating the relevant working directiores, 2) downloading and unzipping the data, 3) reading the data into memory, 4) cleaning and merging the data and finally 5) saving the data to an external file.
* CodeBook.md : Codebook explaining and describes the variables, the data, and all transformations made to the original data
* /Data/*.txt archives: The raw data (this is generated by the run_analysis.R script but is uploaded nontheless to the repo for sake of completeness.
* tidy_dataset.csv :
* tidy_summary_dataset.csv :

## Quick explanation of the script

Fine details on the workings of the script are in the CodeBook.md file, but, quickly explained, it does this:
1) creates the relevant working directiores, called FinalProject and FinalProject/Data
2) downloads and unzips the data, downloads the data from https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip if not present, and unzips it in the FinalProject/Data folder if not done already
3) reads the data into memory, reads and parses the .txt files as fixed width tables
4) cleans and merges the data and finally, clarifies the structure of the tables, merges train and test sets, creates auxiliary columns, cleans variable names and creates derived, summary data sets.
5) saves the data to an external file, for the peer-grading requirements of Coursera

## Description of the data

The data consists of 564 variables on 10299 observations, of which we select 68 variables. The summary data we produce consists of 180 observations on 68 variables resulting from taking the mean by subject and activity of the measures of the mean and standard deviation present in the original variables.

Granularity:

* 30 subjects
* 6 activities
* 2 crossvalidation sets (training and test sets)

Two domains:

* Time
* Frequency

Two subjects:

* Body
* Gravity
  
Measures:

* Acceleration
* Magnitude
* Jerk
* Angular acceleration
and combinations therein...

Dimensions:

* X
* Y
* Z

Statistics:

* mean(): Mean value
* std(): Standard deviation
* mad(): Median absolute deviation 
* max(): Largest value in array
* min(): Smallest value in array
* sma(): Signal magnitude area
* energy(): Energy measure. Sum of the squares divided by the number of values. 
* iqr(): Interquartile range 
* entropy(): Signal entropy
* arCoeff(): Autorregresion coefficients with Burg order equal to 4
* correlation(): correlation coefficient between two signals
* maxInds(): index of the frequency component with largest magnitude
* meanFreq(): Weighted average of the frequency components to obtain a mean frequency
* skewness(): skewness of the frequency domain signal 
* kurtosis(): kurtosis of the frequency domain signal 
* bandsEnergy(): Energy of a frequency interval within the 64 bins of the FFT of each window.
* angle(): Angle between to vectors.

## Transformations made to the data

### DOWNLOADING DATA 

The script first checks for the existence of a directory for our final project, and if not present, it creates one. Later, checks if the .zip is already present, if not, downloads the relevant data and unzips it. I know there is some redundancy in the folder structure but I can't still get 
 
 If the program detects that this is done, it does not do it again and continues to the next step.

### LOADING DATA 

 If you see the resulting directories, there are multitude of .txt files.
 I did the work of going one by one to see what each one was about

 First there is label data.

 there are many, many features, so a "by-hand" variable naming scheme
 is probably not a good idea. I guess what we need to do is programmatically
 cook some (more) descriptive names for the variables. We're lucky the data is
 regular enough for simple tricks to be enough

 Then the data itself

 Testing set

 Training set

 with its associated subjects

 NOTE: The folders on inertial signals will be discarded later by the exercise
 itself so we don't concern ourselves with them.

### DATABASE CONSTRUCTION 

 First, let's assign variable names according to our source material. I prefer
 doing it all two times (before the merge) to lessen the chance that
 I overlook something and the merge ends up with some subtle error.
 Better to tidyly merge two tidy datasets than tidying a dirty merged dataset.

 Now, let's convert our activity data into factors with the corresponding
 labels according to 'activity_labels'. Casting it directly with 'factor'
 leaves us with a factor vector. I force a data.frame and name the variable.

 Now we can peacefully merge subject, X, and y

 Then, I consider important remembering from where (of the crossvalidation step)
 does the data come from (in general, although in this exercise it does not matter):

### MERGING DATASETS 

 We 'merge' (that is, append the records of one to the other) the data sets to end up
 with the final data set.

 We can now touch up some little details we left out in the data

 Tidying up we got with the suprise that there are duplicated column names
 that come from the very source material 'feature_names.txt':

 We fix this with repair_names(). No measures with mean or std are affected so there's no
 cause for confusion.

 We select either:

 OPTION 1: Everything that has 'mean' or 'std' in its name is selected, or

 OPTION 2: Only variables with calculated mean() and std() in their names are selected

 This feels more consistent, as it seems conceptually more correct to think
 that this:
 "Extracts only the measurements on the mean and standard deviation for each measurement."
 meant all .mean() and std() (derived measures) than selecting
 simply those which happen to have mean or std in their names.

### DERIVED DATASETS 

 We calculate the derived measure means by subject and activity. This produces means of means
 and means of standard errors but this makes sense in a bayesian context.

### DESCRIPTIVE NAMES:

 Finally, given that the feature names are relatively expressive, we can
 clarify that t is time domain, acc is acceleration, Gyro is angular acceleration (gyroscope)
 f is frequency domain and Mag is magnitude. We do this with regular expressions.
 We also standarize the colnames as per usual R conventions. I know the prof. uses sometimes
 dots in the names (as in Google's R style guide) and discourages "_" in colnames but the
 R style guide itself separating with _ in names is encouraged, so I replace camelCase with
 underscore_separation and replace '-' with '_'
 
### EXPORTING DATA:

 It is not clear which data set we must upload to coursera so we save both.
 We write to .csv because parsing is usually easier and there is no
 possibility of mistaking the comma with some comma in the data.

 As the summary_data is the result of more transformations
 than the final_HAR_data, I assume that this is what we
 should upload to Coursera to show our work.

# Technical details

Only base R and the tidyverse is used throughout the script. I suppose there are better ways of expressing
the regular expressions used when modifying the column names but they are good enough as they are.
I guess a slightly different reordering of the transformation steps would've been better in terms
of time and memory but the data set is small enough for it to not matter.
